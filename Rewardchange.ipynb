{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Rewardchange",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4e6Qy24rtQxc"
      },
      "source": [
        "import argparse\n",
        "import copy\n",
        "import dataclasses\n",
        "import datetime\n",
        "import os\n",
        "import time\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import rsoccer_gym\n",
        "import torch.multiprocessing as mp\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import wandb\n",
        "from agents.ddpg import (DDPGHP, DDPGActor, DDPGCritic, TargetActor,\n",
        "                         TargetCritic, data_func)\n",
        "from agents.utils import ReplayBuffer, save_checkpoint, unpack_batch, ExperienceFirstLast\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    mp.set_start_method('spawn')\n",
        "    os.environ['OMP_NUM_THREADS'] = \"1\"\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--cuda\", default=False,\n",
        "                        action=\"store_true\", help=\"Enable cuda\")\n",
        "    parser.add_argument(\"-n\", \"--name\", required=True,\n",
        "                        help=\"Name of the run\")\n",
        "    parser.add_argument(\"-e\", \"--env\", required=True,\n",
        "                        help=\"Name of the gym environment\")\n",
        "    args = parser.parse_args()\n",
        "    device = \"cuda\" if args.cuda else \"cpu\"\n",
        "\n",
        "    # Input Experiment Hyperparameters\n",
        "    hp = DDPGHP(\n",
        "        EXP_NAME=args.name,\n",
        "        DEVICE=device,\n",
        "        ENV_NAME=args.env,\n",
        "        N_ROLLOUT_PROCESSES=3,\n",
        "        LEARNING_RATE=0.0001,\n",
        "        EXP_GRAD_RATIO=10,\n",
        "        BATCH_SIZE=256,\n",
        "        GAMMA=0.95,\n",
        "        REWARD_STEPS=3,\n",
        "        NOISE_SIGMA_INITIAL=0.8,\n",
        "        NOISE_THETA=0.15,\n",
        "        NOISE_SIGMA_DECAY=0.99,\n",
        "        NOISE_SIGMA_MIN=0.15,\n",
        "        NOISE_SIGMA_GRAD_STEPS=3000,\n",
        "        REPLAY_SIZE=1000000,\n",
        "        REPLAY_INITIAL=100000,\n",
        "        SAVE_FREQUENCY=100000,\n",
        "        GIF_FREQUENCY=10000,\n",
        "        TOTAL_GRAD_STEPS=2000000,\n",
        "        MULTI_AGENT=True\n",
        "    )\n",
        "    wandb.init(project='RoboCIn-RL', name=hp.EXP_NAME, config=hp.to_dict())\n",
        "    current_time = datetime.datetime.now().strftime('%b-%d_%H-%M-%S')\n",
        "    tb_path = os.path.join('runs', current_time + '_'\n",
        "                           + hp.ENV_NAME + '_' + hp.EXP_NAME)\n",
        "\n",
        "    pi = DDPGActor(hp.N_OBS, hp.N_ACTS).to(device)\n",
        "    Q = DDPGCritic(hp.N_OBS, hp.N_ACTS).to(device)\n",
        "\n",
        "    # Playing\n",
        "    pi.share_memory()\n",
        "    exp_queue = mp.Queue(maxsize=hp.EXP_GRAD_RATIO)\n",
        "    finish_event = mp.Event()\n",
        "    sigma_m = mp.Value('f', hp.NOISE_SIGMA_INITIAL)\n",
        "    gif_req_m = mp.Value('i', -1)\n",
        "    data_proc_list = []\n",
        "    for _ in range(hp.N_ROLLOUT_PROCESSES):\n",
        "        data_proc = mp.Process(\n",
        "            target=data_func,\n",
        "            args=(\n",
        "                pi,\n",
        "                device,\n",
        "                exp_queue,\n",
        "                finish_event,\n",
        "                sigma_m,\n",
        "                gif_req_m,\n",
        "                hp\n",
        "            )\n",
        "        )\n",
        "        data_proc.start()\n",
        "        data_proc_list.append(data_proc)\n",
        "\n",
        "    # Training\n",
        "    tgt_pi = TargetActor(pi)\n",
        "    tgt_Q = TargetCritic(Q)\n",
        "    pi_opt = optim.Adam(pi.parameters(), lr=hp.LEARNING_RATE)\n",
        "    Q_opt = optim.Adam(Q.parameters(), lr=hp.LEARNING_RATE)\n",
        "    buffer = ReplayBuffer(buffer_size=hp.REPLAY_SIZE,\n",
        "                          observation_space=hp.observation_space,\n",
        "                          action_space=hp.action_space,\n",
        "                          device=hp.DEVICE\n",
        "                          )\n",
        "    n_grads = 0\n",
        "    n_samples = 0\n",
        "    n_episodes = 0\n",
        "    best_reward = None\n",
        "    last_gif = None\n",
        "\n",
        "    try:\n",
        "        while n_grads < hp.TOTAL_GRAD_STEPS:\n",
        "            metrics = {}\n",
        "            ep_infos = list()\n",
        "            st_time = time.perf_counter()\n",
        "            # Collect EXP_GRAD_RATIO sample for each grad step\n",
        "            new_samples = 0\n",
        "            while new_samples < hp.EXP_GRAD_RATIO:\n",
        "                exp = exp_queue.get()\n",
        "                if exp is None:\n",
        "                    raise Exception  # got None value in queue\n",
        "                safe_exp = copy.deepcopy(exp)\n",
        "                del(exp)\n",
        "\n",
        "                # Dict is returned with end of episode info\n",
        "                if isinstance(safe_exp, dict):\n",
        "                    logs = {\"ep_info/\"+key: value for key,\n",
        "                            value in safe_exp.items() if 'truncated' not in key}\n",
        "                    ep_infos.append(logs)\n",
        "                    n_episodes += 1\n",
        "                else:\n",
        "                    for exp in safe_exp:\n",
        "                        if exp.last_state is not None:\n",
        "                            last_state = exp.last_state\n",
        "                        else:\n",
        "                            last_state = exp.state\n",
        "                        \n",
        "##########################MUDANÃ‡A NA REWARD##########################3\n",
        "                        if exp.reward > 0:\n",
        "                            buffer.add(\n",
        "                              obs=exp.state,\n",
        "                              next_obs=last_state,\n",
        "                              action=exp.action,\n",
        "                              reward=exp.reward,\n",
        "                              done=False if exp.last_state is not None else True\n",
        "                            )\n",
        "                    new_samples += 1\n",
        "            n_samples += new_samples\n",
        "            sample_time = time.perf_counter()\n",
        "\n",
        "\n",
        "            # Only start training after buffer is larger than initial value\n",
        "            if buffer.size() < hp.REPLAY_INITIAL:\n",
        "                continue\n",
        "\n",
        "            # Sample a batch and load it as a tensor on device\n",
        "            batch = buffer.sample(hp.BATCH_SIZE)\n",
        "            S_v = batch.observations\n",
        "            A_v = batch.actions\n",
        "            r_v = batch.rewards\n",
        "            dones = batch.dones\n",
        "            S_next_v = batch.next_observations\n",
        "\n",
        "            # train critic\n",
        "            Q_opt.zero_grad()\n",
        "            Q_v = Q(S_v, A_v)  # expected Q for S,A\n",
        "            A_next_v = tgt_pi(S_next_v)  # Get an Bootstrap Action for S_next\n",
        "            Q_next_v = tgt_Q(S_next_v, A_next_v)  # Bootstrap Q_next\n",
        "            Q_next_v[dones == 1.] = 0.0  # No bootstrap if transition is terminal\n",
        "            # Calculate a reference Q value using the bootstrap Q\n",
        "            Q_ref_v = r_v + Q_next_v * (hp.GAMMA**hp.REWARD_STEPS)\n",
        "            Q_loss_v = F.mse_loss(Q_v, Q_ref_v.detach())\n",
        "            Q_loss_v.backward()\n",
        "            Q_opt.step()\n",
        "            metrics[\"train/loss_Q\"] = Q_loss_v.cpu().detach().numpy()\n",
        "\n",
        "            # train actor - Maximize Q value received over every S\n",
        "            pi_opt.zero_grad()\n",
        "            A_cur_v = pi(S_v)\n",
        "            pi_loss_v = -Q(S_v, A_cur_v)\n",
        "            pi_loss_v = pi_loss_v.mean()\n",
        "            pi_loss_v.backward()\n",
        "            pi_opt.step()\n",
        "            metrics[\"train/loss_pi\"] = pi_loss_v.cpu().detach().numpy()\n",
        "\n",
        "            # Sync target networks\n",
        "            tgt_pi.sync(alpha=1 - 1e-3)\n",
        "            tgt_Q.sync(alpha=1 - 1e-3)\n",
        "\n",
        "            n_grads += 1\n",
        "            grad_time = time.perf_counter()\n",
        "            metrics['speed/samples'] = new_samples/(sample_time - st_time)\n",
        "            metrics['speed/grad'] = 1/(grad_time - sample_time)\n",
        "            metrics['speed/total'] = 1/(grad_time - st_time)\n",
        "            metrics['counters/samples'] = n_samples\n",
        "            metrics['counters/grads'] = n_grads\n",
        "            metrics['counters/episodes'] = n_episodes\n",
        "            metrics[\"counters/buffer_len\"] = buffer.size()\n",
        "\n",
        "            if ep_infos:\n",
        "                for key in ep_infos[0].keys():\n",
        "                    if isinstance(ep_infos[0][key], dict):\n",
        "                        for i in range(hp.N_AGENTS):\n",
        "                            for inner_key in ep_infos[0][key].keys():\n",
        "                                metrics[f\"ep_info/agent_{i}/{inner_key}\"] = np.mean(\n",
        "                                    [info[key][inner_key] for info in ep_infos])\n",
        "                    else:\n",
        "                        metrics[key] = np.mean([info[key] for info in ep_infos])\n",
        "\n",
        "            # Log metrics\n",
        "            wandb.log(metrics)\n",
        "\n",
        "            if hp.NOISE_SIGMA_DECAY and sigma_m.value > hp.NOISE_SIGMA_MIN \\\n",
        "                and n_grads % hp.NOISE_SIGMA_GRAD_STEPS == 0:\n",
        "                # This syntax is needed to be process-safe\n",
        "                # The noise sigma value is accessed by the playing processes\n",
        "                with sigma_m.get_lock():\n",
        "                    sigma_m.value *= hp.NOISE_SIGMA_DECAY\n",
        "\n",
        "            if hp.SAVE_FREQUENCY and n_grads % hp.SAVE_FREQUENCY == 0:\n",
        "                save_checkpoint(\n",
        "                    hp=hp,\n",
        "                    metrics={\n",
        "                        'noise_sigma': sigma_m.value,\n",
        "                        'n_samples': n_samples,\n",
        "                        'n_episodes': n_episodes,   \n",
        "                        'n_grads': n_grads,\n",
        "                    },\n",
        "                    pi=pi,\n",
        "                    Q=Q,\n",
        "                    pi_opt=pi_opt,\n",
        "                    Q_opt=Q_opt\n",
        "                )\n",
        "\n",
        "            if hp.GIF_FREQUENCY and n_grads % hp.GIF_FREQUENCY == 0:\n",
        "                gif_req_m.value = n_grads\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"...Finishing...\")\n",
        "        finish_event.set()\n",
        "\n",
        "    finally:\n",
        "        if exp_queue:\n",
        "            while exp_queue.qsize() > 0:\n",
        "                exp_queue.get()\n",
        "\n",
        "        print('queue is empty')\n",
        "\n",
        "        print(\"Waiting for threads to finish...\")\n",
        "        for p in data_proc_list:\n",
        "            p.terminate()\n",
        "            p.join()\n",
        "\n",
        "        del(exp_queue)\n",
        "        del(pi)\n",
        "\n",
        "        finish_event.set()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}